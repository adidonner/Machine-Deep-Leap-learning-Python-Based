{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tuQ5OHzswvXljvZkyScB11kaZ6avt1KQ",
      "authorship_tag": "ABX9TyMYC2LmaVu2gaq4pPh3xpq/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adidonner/Machine-Deep-Leap-learning-Python-Based/blob/main/Task7_Cat_vs_Dogs_VGG_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "import glob\n",
        "import cv2\n",
        "import random"
      ],
      "metadata": {
        "id": "YeTrGGGp9hUb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/content/drive/MyDrive/cats_vs_dogs_data'\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory,\n",
        "    labels='inferred',\n",
        "    batch_size=32,\n",
        "    image_size=(150, 150),\n",
        "    shuffle=True,\n",
        "    seed = 5,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory,\n",
        "    labels='inferred',\n",
        "    batch_size=32,\n",
        "    image_size=(150, 150),\n",
        "    shuffle=False,\n",
        "    seed = 5,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkAM2ZkV_4DQ",
        "outputId": "991930c1-dc07-424f-b1c1-a04e4cfd14ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1467 files belonging to 2 classes.\n",
            "Using 1174 files for training.\n",
            "Found 1467 files belonging to 2 classes.\n",
            "Using 293 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take this code and add to it: Freeze the first 3 layers of the VGG model except for FULLY CONNECTED and the last one. Use GPU and architecture - TorchPy. Save  in \"/content/drive/MyDrive\" the network weights"
      ],
      "metadata": {
        "id": "-SnokynKNmLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Build CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
        "\n",
        "# Freeze the first three layers (excluding fully connected and the last one)\n",
        "for layer in model.layers[:6]:  # Adjust the index based on the actual layer structure\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "# Save the weights\n",
        "save_path = \"/content/drive/MyDrive/vgg_model_weights_pytorch.pth\"\n",
        "torch.save(vgg_model.state_dict(), save_path)\n",
        "print(f'Model weights saved to {save_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyKAueVpNzMm",
        "outputId": "2ecc684b-af8e-4379-8cdf-2819df4da419"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "37/37 [==============================] - 65s 2s/step - loss: 58.9283 - accuracy: 0.9216 - val_loss: 1.0391 - val_accuracy: 0.8430\n",
            "Epoch 2/10\n",
            "37/37 [==============================] - 67s 2s/step - loss: 0.3928 - accuracy: 0.9608 - val_loss: 0.3823 - val_accuracy: 0.8737\n",
            "Epoch 3/10\n",
            "37/37 [==============================] - 66s 2s/step - loss: 0.1196 - accuracy: 0.9761 - val_loss: 0.2921 - val_accuracy: 0.9317\n",
            "Epoch 4/10\n",
            "37/37 [==============================] - 70s 2s/step - loss: 0.0355 - accuracy: 0.9898 - val_loss: 0.2052 - val_accuracy: 0.9795\n",
            "Epoch 5/10\n",
            "37/37 [==============================] - 62s 2s/step - loss: 0.0088 - accuracy: 0.9991 - val_loss: 0.3207 - val_accuracy: 0.9761\n",
            "Epoch 6/10\n",
            "37/37 [==============================] - 67s 2s/step - loss: 0.0160 - accuracy: 0.9983 - val_loss: 0.2288 - val_accuracy: 0.9795\n",
            "Epoch 7/10\n",
            "37/37 [==============================] - 61s 2s/step - loss: 0.0344 - accuracy: 0.9949 - val_loss: 0.1763 - val_accuracy: 0.9761\n",
            "Epoch 8/10\n",
            "37/37 [==============================] - 66s 2s/step - loss: 0.0468 - accuracy: 0.9983 - val_loss: 0.1949 - val_accuracy: 0.9761\n",
            "Epoch 9/10\n",
            "37/37 [==============================] - 61s 2s/step - loss: 0.0255 - accuracy: 0.9966 - val_loss: 0.1744 - val_accuracy: 0.9761\n",
            "Epoch 10/10\n",
            "37/37 [==============================] - 65s 2s/step - loss: 0.0587 - accuracy: 0.9932 - val_loss: 0.2586 - val_accuracy: 0.9693\n",
            "Model weights saved to /content/drive/MyDrive/vgg_model_weights_pytorch.pth\n"
          ]
        }
      ]
    }
  ]
}